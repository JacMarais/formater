{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "path = \"gateway/bloomberg/blaw/llm_gateway_client\"\n",
    "\n",
    "comment_symbol = \"#\"\n",
    "lang = \"python\"\n",
    "\n",
    "ignore_dirs = [\"node_modules\", \".git\", \".vscode\", \"__pycache__\"]\n",
    "ignore_files = [\"package-lock.json\"]\n",
    "allowed_extensions = (\".ts\", \".tsx\", \".js\",\".jsx\", \".json\", \".cjs\", \".py\")\n",
    "ignored_extensions = ()\n",
    "\n",
    "print_token_counts = False\n",
    "\n",
    "target_files = []\n",
    "# target_files = []\n",
    "\n",
    "\n",
    "\n",
    "encode = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    "count_token = lambda x: len(encode(x))\n",
    "\n",
    "def generate_tree(start_path, ignore_dirs = ignore_dirs):\n",
    "    tree = \"\"\n",
    "    for root, dirs, files in os.walk(start_path):\n",
    "        # ignore dirs\n",
    "        for ignore_dir in ignore_dirs:\n",
    "            if ignore_dir in dirs:\n",
    "                dirs.remove(ignore_dir)\n",
    "            \n",
    "        level = root.replace(start_path, '').count(os.sep)\n",
    "        indent = ' ' * 1 * (level)\n",
    "        tree += f\"{indent}{os.path.basename(root)}/\\n\"\n",
    "        sub_indent = ' ' * 1 * (level + 1)\n",
    "        for f in files:\n",
    "            tree += f\"{sub_indent}{f}\\n\"\n",
    "    return tree[:-1]\n",
    "\n",
    "def walk_files(path = \"\", target_files=target_files, output_file = \"output.txt\", lang=\"typescript\", ignore_dirs = ignore_dirs, print_token_counts = True, ignore_files = ignore_files, comment_symbol = \"\", allowed_extensions = allowed_extensions, ignored_extensions = ignored_extensions):\n",
    "    if path == \"\": path = os.getcwd()\n",
    "    if comment_symbol == \"\": comment_symbol = \"//\"\n",
    "\n",
    "    zero_target_files = target_files == []\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    outputs = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for ignore_dir in ignore_dirs:\n",
    "            if ignore_dir in dirs:\n",
    "                dirs.remove(ignore_dir)\n",
    "\n",
    "        for file in files:\n",
    "            if file.endswith(allowed_extensions) and not file.endswith(ignored_extensions):\n",
    "                if file in ignore_files:\n",
    "                    continue\n",
    "                if zero_target_files or file in target_files:\n",
    "                    with open(os.path.join(root, file), \"r\") as infile:\n",
    "                        relative_path = f'{root.replace(cwd, \"\")}/{file}'\n",
    "                        # if the relative path starts with a /, remove it\n",
    "                        if relative_path[0] == '/':\n",
    "                            relative_path = relative_path[1:]\n",
    "\n",
    "                        data = infile.read()\n",
    "                        comment_pattern = re.escape(comment_symbol) + '.*$'\n",
    "                        data = re.sub(comment_pattern, ' ', data, flags=re.MULTILINE)\n",
    "                        data = re.sub( r'(?<=\\n)[ \\t]+(?=\\n)', '', data)\n",
    "                        data = re.sub('\\n+', '<newline>', data)\n",
    "\n",
    "                        data = data.replace(';', '; ')\n",
    "                        data = re.sub(r' +', ' ', data)\n",
    "\n",
    "                        if print_token_counts:\n",
    "                            token_count = count_token(data) + count_token(relative_path)\n",
    "                        else:\n",
    "                            token_count = 0\n",
    "                                            \n",
    "                        output = {\n",
    "                            \"path\": relative_path,\n",
    "                            \"token_count\": token_count,\n",
    "                            \"data\": data\n",
    "                        }\n",
    "\n",
    "                        if len(data) > 0:\n",
    "                            outputs.append(output)\n",
    "\n",
    "    \n",
    "    outputs = sorted(outputs, key=lambda x: x['path'].split('.')[-1])\n",
    "    sept = '\\n\\n---\\n\\n'\n",
    "    tree = generate_tree(path) + sept\n",
    "\n",
    "    total_token_count = count_token(''.join([x['data'] for x in outputs]) + tree)\n",
    "\n",
    "    # output_content = f'```{lang}\\n{data}\\n```'\n",
    "    generate_output_content = lambda x: f\"```{lang}\\n{comment_symbol} {x['path']}\\n{x['data']}\\n```\"\n",
    "\n",
    "    if print_token_counts:\n",
    "        out_string = tree + sept.join([f\"{x['token_count']}\\n{generate_output_content(x)}\" for x in outputs])\n",
    "    else:\n",
    "        out_string = tree + sept.join([generate_output_content(x) for x in outputs])\n",
    "\n",
    "    with open(output_file, \"w\") as outfile:\n",
    "        outfile.write(f'Total token count: {total_token_count}\\n\\n{out_string}')\n",
    "\n",
    "walk_files(path, print_token_counts = print_token_counts, comment_symbol = comment_symbol, lang = lang)\n",
    "os.system(\"code output.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
